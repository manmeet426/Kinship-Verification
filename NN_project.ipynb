{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Would be trying VGG16 as a convolutional base to further increase performance accuracy of the network.\n",
    "\n",
    "from keras.applications import VGG16\n",
    "\n",
    "conv_base = VGG16(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(64, 64, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x25565aa69b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# The load data function loads the files(images) from all the classes (f-d, f-s, m-d, m-s), prepares labels for the same\n",
    "# and calls import_dir which performs the function to merge the parent and child images into a single image as required for\n",
    "# input to the model.\n",
    "# returns all images from all folders into parent-child image form.\n",
    "def load_data(dirs):\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    combination= {}\n",
    "    label = 1\n",
    "    for dir in sorted(os.listdir(dirs)):\n",
    "        dir_name = os.path.join(dirs, dir)\n",
    "        images, num_of_files = import_dir(dir_name)\n",
    "        all_images.append(images)\n",
    "        all_labels.append([label for i in range(num_of_files)])\n",
    "        print(dir_name)\n",
    "        print(len(images))\n",
    "        label += 1\n",
    "    return all_images, all_labels \n",
    "\n",
    "\n",
    "# import_dir reads all files(images) for a given directory (say father-dau) and combines parent-child images to form a single\n",
    "# 6 channel image which is used as input for the proposed model.\n",
    "# The start variable is required to skip any thumbs.db file that may be present in the folder containing the images.\n",
    "# return parent-child images for the given directory. \n",
    "\n",
    "def import_dir(dir_name):\n",
    "    files = sorted(os.listdir(dir_name))\n",
    "    dir_images = []\n",
    "    start = 0\n",
    "    if len(files)%2 != 0:\n",
    "        start = 1\n",
    "    for file1, file2 in zip(files[start::2], files[start+1::2]):\n",
    "        images = np.zeros((64, 64, 6))\n",
    "        #if file.split('.')[0][-1] == \"1\":\n",
    "        parent = plt.imread(os.path.join(dir_name, file1))\n",
    "        images[:,:,0:3] = parent\n",
    "        #if file.split('.')[0][-1] == \"2\":\n",
    "        child = plt.imread(os.path.join(dir_name, file2))\n",
    "        images[:,:,3:6] = child\n",
    "#       plt.imshow(images[:,:,5])\n",
    "#       plt.show()\n",
    "#       input(\" .... \")\n",
    "#       print(file)\n",
    "        dir_images.append(images)\n",
    "      \n",
    "    return dir_images, len(dir_images)\n",
    "\n",
    "dir_images, length = import_dir(\"C:\\\\Users\\\\Manmeet\\\\Desktop\\\\Spring 2019\\\\NN\\\\Project\\\\KinFaceW-II\\\\KinFaceW-II\\\\images\\\\father-dau\")\n",
    "plt.imshow(dir_images[78][:,:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for another possible implementation to be tried later to get better performance\n",
    "\n",
    "# def import_parent(dir_name):\n",
    "#     files = sorted(os.listdir(dir_name))\n",
    "#     dir_images = []\n",
    "#     start = 0\n",
    "#     if len(files)%2 != 0:\n",
    "#         start = 1\n",
    "#     for file in files[start::2]:\n",
    "#         image = np.zeros((64, 64, 3))\n",
    "#         #if file.split('.')[0][-1] == \"1\":\n",
    "#         parent = plt.imread(os.path.join(dir_name, file))\n",
    "#         image[:,:,0:3] = parent\n",
    "#         #if file.split('.')[0][-1] == \"2\":\n",
    "# #         child = plt.imread(os.path.join(dir_name, file2))\n",
    "# #         images[:,:,3:6] = child\n",
    "# #       plt.imshow(images[:,:,5])\n",
    "# #       plt.show()\n",
    "# #       input(\" .... \")\n",
    "# #       print(file)\n",
    "#         dir_images.append(image)\n",
    "      \n",
    "#     return dir_images, len(dir_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for another possible implementation to be tried later to get better performance\n",
    "\n",
    "# def import_child(dir_name):\n",
    "#     files = sorted(os.listdir(dir_name))\n",
    "#     dir_images = []\n",
    "#     start = 0\n",
    "#     if len(files)%2 != 0:\n",
    "#         start = 1\n",
    "#     for file in files[start+1::2]:\n",
    "#         image = np.zeros((64, 64, 3))\n",
    "#         #if file.split('.')[0][-1] == \"1\":\n",
    "# #         parent = plt.imread(os.path.join(dir_name, file1))\n",
    "# #         images[:,:,0:3] = parent\n",
    "#         #if file.split('.')[0][-1] == \"2\":\n",
    "#         child = plt.imread(os.path.join(dir_name, file))\n",
    "#         image[:,:,0:3] = child\n",
    "# #       plt.imshow(images[:,:,5])\n",
    "# #       plt.show()\n",
    "# #       input(\" .... \")\n",
    "# #       print(file)\n",
    "#         dir_images.append(image)\n",
    "      \n",
    "#     return dir_images, len(dir_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for another possible implementation to be tried later to get better performance\n",
    "\n",
    "# def load_parent_child_images(dirs):\n",
    "#     parent_images = []\n",
    "#     child_images = []\n",
    "#     for dir in sorted(os.listdir(dirs)):\n",
    "#         dir_name = os.path.join(dirs, dir)\n",
    "#         parent_image, num_of_parent_files = import_parent(dir_name)\n",
    "#         parent_images.append(parent_image)\n",
    "#         child_image, num_of_child_files = import_child(dir_name)\n",
    "#         child_images.append(child_image)\n",
    "#         print(dir_name)\n",
    "#         print(len(parent_image))\n",
    "#         print(len(child_image))\n",
    "#     return parent_images, child_images    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for another possible implementation to be tried later to get better performance\n",
    "\n",
    "# parents, children = load_parent_child_images(\"C:\\\\Users\\\\Manmeet\\\\Desktop\\\\Spring 2019\\\\NN\\\\Project\\\\KinFaceW-II\\\\KinFaceW-II\\\\images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for another possible implementation to be tried later to get better performance\n",
    "\n",
    "# labels contains labels for both parent and child images.\n",
    "\n",
    "# iterator = 0\n",
    "# labels = []\n",
    "# for images in parents:\n",
    "#     for item in images:\n",
    "#         labels.append(iterator)\n",
    "#     iterator += 1\n",
    "# print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for another possible implementation to be tried later to get better performance\n",
    "\n",
    "# # Pass the parent images through the designed network.\n",
    "# from keras import layers\n",
    "# from keras import models\n",
    "# from keras import optimizers\n",
    "\n",
    "# def parent_model():\n",
    "#     model = models.Sequential()\n",
    "#     model.add(layers.Conv2D(64, (5, 5), strides=1, padding='valid', activation='relu', input_shape=(64, 64, 3)))\n",
    "#     model.add(layers.MaxPooling2D((2, 2)))\n",
    "#     model.add(layers.Conv2D(128, (5, 5), activation='relu'))\n",
    "#     model.add(layers.MaxPooling2D((2, 2)))\n",
    "#     model.add(layers.Conv2D(25, (2, 2), activation='relu'))\n",
    "#     model.add(layers.MaxPooling2D((2, 2)))\n",
    "#     model.add(layers.Conv2D(512, (5, 5), activation='relu'))\n",
    "#     model.add(layers.MaxPooling2D((2, 2)))\n",
    "# #     model.add(layers.Flatten())\n",
    "# #     model.add(layers.Dense(512, activation=layers.ReLU(max_value=1.0)))\n",
    "# # #model.add(layers.Dropout(0.4))\n",
    "# #     model.add(layers.Dense(4, activation='softmax'))\n",
    "# #     sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "# #     model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for another possible implementation to be tried later to get better performance\n",
    "\n",
    "# par_model = parent_model()\n",
    "# print(par_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for another possible implementation to be tried later to get better performance\n",
    "\n",
    "# def layer2_model():\n",
    "#     model = models.Sequential()\n",
    "#     model.add(layers.Conv2D(64, (5, 5), strides=1, padding='valid', activation='relu', input_shape=(64, 64, 3)))\n",
    "#     model.add(layers.MaxPooling2D((2, 2)))\n",
    "#     model.add(layers.Conv2D(128, (5, 5), activation='relu'))\n",
    "#     model.add(layers.MaxPooling2D((2, 2)))\n",
    "#     model.add(layers.Conv2D(25, (2, 2), activation='relu'))\n",
    "#     model.add(layers.MaxPooling2D((2, 2)))\n",
    "#     model.add(layers.Conv2D(512, (5, 5), activation='relu'))\n",
    "#     model.add(layers.MaxPooling2D((2, 2)))\n",
    "# #     model.add(layers.Flatten())\n",
    "# #     model.add(layers.Dense(512, activation=layers.ReLU(max_value=1.0)))\n",
    "# # #model.add(layers.Dropout(0.4))\n",
    "# #     model.add(layers.Dense(4, activation='softmax'))\n",
    "# #     sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "# #     model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manmeet\\Desktop\\Spring 2019\\NN\\Project\\KinFaceW-II\\KinFaceW-II\\images\\father-dau\n",
      "250\n",
      "C:\\Users\\Manmeet\\Desktop\\Spring 2019\\NN\\Project\\KinFaceW-II\\KinFaceW-II\\images\\father-son\n",
      "250\n",
      "C:\\Users\\Manmeet\\Desktop\\Spring 2019\\NN\\Project\\KinFaceW-II\\KinFaceW-II\\images\\mother-dau\n",
      "250\n",
      "C:\\Users\\Manmeet\\Desktop\\Spring 2019\\NN\\Project\\KinFaceW-II\\KinFaceW-II\\images\\mother-son\n",
      "250\n"
     ]
    }
   ],
   "source": [
    "# Using the load_data function obtain the train images (in paernt-child format) along with the labels for each class\n",
    "# from the KinFaceW-II dataset.\n",
    "train_images, train_labels = load_data(\"C:\\\\Users\\\\Manmeet\\\\Desktop\\\\Spring 2019\\\\NN\\\\Project\\\\KinFaceW-II\\\\KinFaceW-II\\\\images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all train_images obtained into a single variable train_img\n",
    "# Combine all train_labels obtained into a single variable train_labels\n",
    "train_img = np.r_[train_images[0], train_images[1], train_images[2], train_images[3]]\n",
    "train_labels = np.r_[np.array(train_labels[0]), np.array(train_labels[1]), np.array(train_labels[2]), np.array(train_labels[3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manmeet\\Desktop\\Spring 2019\\NN\\Project\\KinFaceW-I\\KinFaceW-I\\images\\father-dau\n",
      "134\n",
      "C:\\Users\\Manmeet\\Desktop\\Spring 2019\\NN\\Project\\KinFaceW-I\\KinFaceW-I\\images\\father-son\n",
      "156\n",
      "C:\\Users\\Manmeet\\Desktop\\Spring 2019\\NN\\Project\\KinFaceW-I\\KinFaceW-I\\images\\mother-dau\n",
      "127\n",
      "C:\\Users\\Manmeet\\Desktop\\Spring 2019\\NN\\Project\\KinFaceW-I\\KinFaceW-I\\images\\mother-son\n",
      "116\n"
     ]
    }
   ],
   "source": [
    "# Obtain the test images (in parent-child form) along with the labels for each class\n",
    "# from the KinFaceW-I dataset.\n",
    "test_images, test_labels = load_data(\"C:\\\\Users\\\\Manmeet\\\\Desktop\\\\Spring 2019\\\\NN\\\\Project\\\\KinFaceW-I\\\\KinFaceW-I\\\\images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(533,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine all the test images into a single variable test_imgs\n",
    "# Combine all the test labels into a single variable test_lab\n",
    "\n",
    "test_img = np.zeros((len(test_images[0]) + len(test_images[1]) + len(test_images[2]) + len(test_images[3]), 64, 64, 6))\n",
    "test_img = np.r_[test_images[0], test_images[1], test_images[2], test_images[3]]\n",
    "test_lab = np.r_[np.array(test_labels[0]), np.array(test_labels[1]), np.array(test_labels[2]), np.array(test_labels[3])]\n",
    "test_lab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the labels into categorical variables for the multi-class classification task.\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "train_labels_cat = to_categorical(train_labels - 1)\n",
    "test_labels_cat = to_categorical(test_lab - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manmeet\\AppData\\Local\\conda\\conda\\envs\\cs670\\lib\\site-packages\\keras_preprocessing\\image.py:1358: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3 or 4 channels on axis 3. However, it was passed an array with shape (1000, 64, 64, 6) (6 channels).\n",
      "  ' channels).')\n"
     ]
    }
   ],
   "source": [
    "# Prepare datagenerator for Data Augmetnation\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  \n",
    "        samplewise_center=False,  \n",
    "        featurewise_std_normalization=False,  \n",
    "        samplewise_std_normalization=False,  \n",
    "        zca_whitening=False,  \n",
    "        rotation_range=10,  \n",
    "        zoom_range = 0.0,  \n",
    "        width_shift_range=0.1,  \n",
    "        height_shift_range=0.1,  \n",
    "        horizontal_flip=True) \n",
    "\n",
    "datagen.fit(train_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One possible variation for the proposed network architecture which results in lower performance.\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "\n",
    "# def model_1():\n",
    "#     model = models.Sequential()\n",
    "#     model.add(layers.Conv2D(16, (5, 5), strides=1, padding='valid', activation=layers.ReLU(max_value=0.5), input_shape=(64, 64, 6)))\n",
    "#     model.add(layers.MaxPooling2D((2, 2)))\n",
    "#     model.add(layers.Conv2D(32, (5, 5), activation=layers.ReLU(max_value=0.5)))\n",
    "#     model.add(layers.MaxPooling2D((2, 2)))\n",
    "#     model.add(layers.Conv2D(64, (5, 5), activation=layers.ReLU(max_value=0.5)))\n",
    "#     model.add(layers.MaxPooling2D((2, 2)))\n",
    "#     model.add(layers.Conv2D(64, (2, 2), activation=layers.ReLU(max_value=1.0)))\n",
    "#     model.add(layers.MaxPooling2D((2, 2)))\n",
    "#     model.add(layers.Flatten())\n",
    "#     model.add(layers.Dense(512, activation=layers.ReLU(max_value=1.0)))\n",
    "# #model.add(layers.Dropout(0.4))\n",
    "#     model.add(layers.Dense(4, activation='softmax'))\n",
    "#     sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#     model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proposed architecture for the neural network.\n",
    "\n",
    "from keras import models\n",
    "def model_2():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(16, (5, 5), strides=1, padding='valid', input_shape=(64, 64, 6), use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(layers.Conv2D(32, (3, 3), use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "#     model.add(layers.Dropout(0.4))\n",
    "    \n",
    "    model.add(layers.Conv2D(64, (3, 3), use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    \n",
    "    model.add(layers.Conv2D(64, (2, 2), use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(512, use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    \n",
    "    model.add(layers.Dropout(0.4))\n",
    "    model.add(layers.Dense(4, activation='softmax'))\n",
    "    sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "40/40 [==============================] - 25s 631ms/step - loss: 0.6818 - acc: 0.6985 - val_loss: 0.6330 - val_acc: 0.7101\n",
      "Epoch 2/7\n",
      "40/40 [==============================] - 25s 619ms/step - loss: 0.6058 - acc: 0.7211 - val_loss: 0.6702 - val_acc: 0.6660\n",
      "Epoch 3/7\n",
      "40/40 [==============================] - 23s 568ms/step - loss: 0.6092 - acc: 0.7225 - val_loss: 0.5533 - val_acc: 0.7519\n",
      "Epoch 4/7\n",
      "40/40 [==============================] - 24s 600ms/step - loss: 0.5758 - acc: 0.7322 - val_loss: 0.7125 - val_acc: 0.6646\n",
      "Epoch 5/7\n",
      "40/40 [==============================] - 23s 574ms/step - loss: 0.5564 - acc: 0.7391 - val_loss: 0.5813 - val_acc: 0.7223\n",
      "Epoch 6/7\n",
      "40/40 [==============================] - 23s 564ms/step - loss: 0.5579 - acc: 0.7412 - val_loss: 0.6761 - val_acc: 0.7134\n",
      "Epoch 7/7\n",
      "40/40 [==============================] - 23s 566ms/step - loss: 0.5229 - acc: 0.7536 - val_loss: 0.6840 - val_acc: 0.7012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25536f03be0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the training data with the Data generator to perform Data Augmentation.\n",
    "# Then, fit the model for the normalized train data.\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  \n",
    "        samplewise_center=False,  \n",
    "        featurewise_std_normalization=False,  \n",
    "        samplewise_std_normalization=False,  \n",
    "        zca_whitening=False,  \n",
    "        rotation_range=10,  \n",
    "        zoom_range = 0.0,  \n",
    "        width_shift_range=0.1,  \n",
    "        height_shift_range=0.1,  \n",
    "        horizontal_flip=True) \n",
    "\n",
    "datagen.fit(train_img)\n",
    "model = model_2()\n",
    "model.fit_generator(datagen.flow(train_img/255, train_labels_cat), steps_per_epoch=40, epochs=7, verbose=1, shuffle=True, validation_data=(test_img/255, test_labels_cat), validation_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "533/533 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5525431833727946, 0.75]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the performance as the accuracy on the test set.\n",
    "\n",
    "model.evaluate(test_img/255, test_labels_cat, batch_size=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"improved_model_with_batch_normalzation_and_tanh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the above model is saved as model_with_data_augmentation in the same root file structure.\n",
    "\n",
    "model.save(\"model_with_data_augmentation\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x23493f83940>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.load_model(\"model_with_data_augmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = models.load_model(\"improved_model_with_batch_normalzation_and_tanh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "533/533 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.141733029993569, 0.7528142565634193]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.evaluate(test_img/255, test_labels_cat, batch_size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procedure to convert input images from the GUI into the required input shape for the model.(parent-child 6-channel combination)\n",
    "\n",
    "# image = np.zeros((64, 64, 6))\n",
    "# #if file.split('.')[0][-1] == \"1\":\n",
    "# parent = plt.imread(\"C:\\\\Users\\\\Manmeet\\\\Desktop\\\\Spring 2019\\\\NN\\\\Project\\\\KinFaceW-I\\\\KinFaceW-I\\\\images\\\\mother-son\\\\ms_004_1.jpg\")\n",
    "# image[:,:,0:3] = parent\n",
    "# #if file.split('.')[0][-1] == \"2\":\n",
    "# child = plt.imread(\"C:\\\\Users\\\\Manmeet\\\\Desktop\\\\Spring 2019\\\\NN\\\\Project\\\\KinFaceW-I\\\\KinFaceW-I\\\\images\\\\mother-son\\\\ms_004_2.jpg\")\n",
    "# image[:,:,3:6] = child\n",
    "\n",
    "# x= model.predict_classes(image.reshape(1,64,64,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected: C:/Users/Manmeet/Desktop/Spring 2019/NN/Project/KinFaceW-I/KinFaceW-I/images/father-son/fs_001_1.jpg\n",
      "Selected: C:/Users/Manmeet/Desktop/Spring 2019/NN/Project/KinFaceW-I/KinFaceW-I/images/father-son/fs_001_2.jpg\n",
      "Selected: C:/Users/Manmeet/Desktop/Spring 2019/NN/Project/KinFaceW-I/KinFaceW-I/images/father-dau/fd_004_1.jpg\n",
      "Selected: C:/Users/Manmeet/Desktop/Spring 2019/NN/Project/KinFaceW-I/KinFaceW-I/images/father-dau/fd_004_2.jpg\n"
     ]
    }
   ],
   "source": [
    "# GUI code \n",
    "\n",
    "from tkinter import *\n",
    "from PIL import ImageTk, Image\n",
    "from tkinter import filedialog\n",
    "from tkinter import messagebox\n",
    "\n",
    "# self.x['state'] = 'normal'\n",
    "\n",
    "\n",
    "# Controls the action for the first Upload button\n",
    "def UploadAction1(event=None):\n",
    "    filename = filedialog.askopenfilename()\n",
    "    button2['state'] = 'normal'\n",
    "    print('Selected:', filename)\n",
    "    global f1     \n",
    "    img = Image.open(filename)\n",
    "    img = img.resize((64, 64), Image.ANTIALIAS)\n",
    "    photo = ImageTk.PhotoImage(img)\n",
    "    \n",
    "    label = Label(image=photo)\n",
    "    label.image = photo # keep a reference!\n",
    "    label.grid(column=0, row=3)\n",
    "#     canvas.create_image(20,20, anchor=SW, image=img)\n",
    "\n",
    "    # Store the path of the selected file into a global variable, since we need to use it to form the combination of input images\n",
    "    # to pass as input to the model.\n",
    "    f1 = filename\n",
    "    \n",
    "# Controls the action for the second Upload button\n",
    "def UploadAction2(event=None):\n",
    "    filename = filedialog.askopenfilename()\n",
    "    global f2\n",
    "    print('Selected:', filename)\n",
    "    submit['state'] = 'normal'\n",
    "    # filename2 = filename.replace('/','\\\\')\n",
    "    img = Image.open(filename)\n",
    "    img = img.resize((64, 64), Image.ANTIALIAS)\n",
    "    photo = ImageTk.PhotoImage(img)\n",
    "    \n",
    "    label = Label(image=photo)\n",
    "    label.image = photo # keep a reference!\n",
    "    label.grid(column=10, row=3)\n",
    "    \n",
    "    # Store the path of the selected file into a global variable, since we need to use it to form the combination of input images\n",
    "    # to pass as input to the model.\n",
    "    f2 = filename\n",
    "     \n",
    "# Controls the action of the submit button, which triggers the formation of the input image to the model.\n",
    "# It displays the prediction in a separate dialog box.\n",
    "def SubmitAction():\n",
    "    \n",
    "    # form the parent-child image from the uploaded files\n",
    "    image = np.zeros((64, 64, 6))\n",
    "    #if file.split('.')[0][-1] == \"1\":\n",
    "    parent = plt.imread(f1)\n",
    "    image[:,:,0:3] = parent\n",
    "    #if file.split('.')[0][-1] == \"2\":\n",
    "    child = plt.imread(f2)\n",
    "    image[:,:,3:6] = child\n",
    "    \n",
    "    # pass this input to the model to make a prediction\n",
    "    x = model.predict_classes(image.reshape(1,64,64,6))\n",
    "    result = \"\"\n",
    "    \n",
    "    # Interpret the result based on the value returned by the predict_classes function\n",
    "    if(x[0] == 0):\n",
    "        result = \"The relationship is father-daughter\"\n",
    "    elif(x[0] == 1):\n",
    "        result = \"The relationship is father-son\"\n",
    "    elif(x[0] == 2):\n",
    "        result = \"The relationship is mother-daughter\"\n",
    "    elif(x[0] == 3):\n",
    "        result = \"The relationship is mother-son\"\n",
    "    \n",
    "    # Display the class result in a new dialog box.\n",
    "    msg = messagebox.showinfo( \"Classification\", result)\n",
    "    \n",
    "window = Tk()\n",
    "# to rename the title of the window\n",
    "window.title(\"GUI\")\n",
    "window.geometry(\"300x200\")\n",
    "window.configure(background='grey')\n",
    "\n",
    "button1 = Button(window, text='Upload Image 1', command=UploadAction1)\n",
    "# button1.pack(side=LEFT, fill=None, expand=True, padx=4, pady=1)\n",
    "button1.grid(column=0, padx=10, pady=4, row=1)\n",
    "\n",
    "\n",
    "button2 = Button(window, text='Upload Image 2', command=UploadAction2, state=DISABLED)\n",
    "button2.grid(column = 10, padx=10, pady=4, row=1)\n",
    "\n",
    "# canvas = Canvas(window, width = 300, height = 300)      \n",
    "# canvas.pack()      \n",
    "# img = PhotoImage(file=filename1)      \n",
    "# canvas.create_image(20,20, anchor=NW, image=img) \n",
    "\n",
    "submit = Button(window, text='Submit', command=SubmitAction, state=DISABLED)\n",
    "submit.grid(column = 3, row= 5)\n",
    "    \n",
    "# pack is used to show the object in the window\n",
    "# label = tkinter.Label(window, text = \"Hello World!\").pack()\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
